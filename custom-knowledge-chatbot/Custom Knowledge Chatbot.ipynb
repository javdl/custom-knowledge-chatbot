{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6337c0b",
   "metadata": {},
   "source": [
    "# Custom Knowledge Chatbot w/ LlamaIndex\n",
    "By Joost van der Laan &mdash; example from Liam Ottley - YouTube: https://www.youtube.com/@LiamOttley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c425f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade pip\n",
    "!pip install --quiet --upgrade llama_index\n",
    "!pip install --quiet --upgrade langchain\n",
    "!pip install --quiet --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041eae8",
   "metadata": {},
   "source": [
    "# Basic LlamaIndex Usage Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6395b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = \"sk-\"\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf41880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load you data into 'Documents' a custom type by LlamaIndex\n",
    "\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('./data').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98df5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index of your documents\n",
    "\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18731b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I think Facebook's LLaMa is a great step forward in democratizing access to large language models and advancing research in this subfield of AI. It is encouraging to see that they are making the model available at several sizes and providing a model card to detail how it was built in accordance with responsible AI practices. I also appreciate that they are releasing the model under a noncommercial license focused on research use cases and granting access to academic researchers, government, civil society, and industry research laboratories. This will help ensure that the model is used responsibly and with integrity.\n"
     ]
    }
   ],
   "source": [
    "# Query index\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "response = query_engine.query(\"What do you think of Facebook's LLaMa?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57864389",
   "metadata": {},
   "source": [
    "# Customize your LLM for different output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c726dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your LLM\n",
    "\n",
    "from llama_index import LLMPredictor, GPTVectorStoreIndex, PromptHelper\n",
    "from langchain import OpenAI\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.1, model_name=\"text-davinci-002\"))\n",
    "\n",
    "# define prompt helper\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "custom_LLM_index = GPTVectorStoreIndex.from_documents(\n",
    "    documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf75f64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359b0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 1369 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I think it's a great idea!\n"
     ]
    }
   ],
   "source": [
    "# Query your index!\n",
    "\n",
    "response = custom_LLM_index.query(\"What do you think of Facebook's LLaMa?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f4e5c",
   "metadata": {},
   "source": [
    "# Wikipedia Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4db9772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia~=1.4 (from -r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/wikipedia/requirements.txt (line 1))\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting beautifulsoup4 (from wikipedia~=1.4->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/wikipedia/requirements.txt (line 1))\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.0/143.0 kB 11.7 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.0.0 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from wikipedia~=1.4->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/wikipedia/requirements.txt (line 1)) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia~=1.4->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/wikipedia/requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia~=1.4->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/wikipedia/requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia~=1.4->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/wikipedia/requirements.txt (line 1)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.0.0->wikipedia~=1.4->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/wikipedia/requirements.txt (line 1)) (2023.5.7)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->wikipedia~=1.4->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/wikipedia/requirements.txt (line 1))\n",
      "  Downloading soupsieve-2.4.1-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml): started\n",
      "  Building wheel for wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=3b9034f4c3ad05dcb70bc661d6dc228e5be78556b58eee569ae59a0c86404ea8\n",
      "  Stored in directory: /home/joost/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: soupsieve, beautifulsoup4, wikipedia\n",
      "Successfully installed beautifulsoup4-4.12.2 soupsieve-2.4.1 wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "\n",
    "loader = WikipediaReader()\n",
    "wikidocs = loader.load_data(pages=['Cyclone Freddy'])\n",
    "\n",
    "# https://en.wikipedia.org/wiki/Cyclone_Freddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "941c9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_index = GPTVectorStoreIndex.from_documents(wikidocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4dd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 3844 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 8 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Cyclone Freddy is a very intense tropical cyclone that affected the Mascarene Islands, Madagascar, Mozambique, and Zimbabwe in February 2023. It is the longest-lived tropical cyclone on record, surpassing Hurricane John's record of 31 days. Freddy was once a powerful cyclone that was classified as a Category 5-equivalent tropical cyclone by the Joint Typhoon Warning Center (JTWC). It caused widespread damage and at least 29 deaths in Madagascar, Mozambique, and Zimbabwe. In Madagascar, over 14,000 homes were affected, with 5,500 destroyed, 3,079 flooded, and at least 9,696 damaged. At least 24,358 people were displaced, and nearly 25,000 customers were left without power at the height of the cyclone. In Saint-Paul, 20 tons of mangoes were destroyed, and Highway RD48 in Salazie was closed due to a landslide. Eleven mobile sites maintained by Orange S.A. were knocked offline in Tampon, Saint-Louis, and Saint-Paul.\n"
     ]
    }
   ],
   "source": [
    "response = wiki_index.query(\"What is cyclone freddy?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2acc",
   "metadata": {},
   "source": [
    "# Customer Support Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19f396a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./asos').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb30944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "946c44ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In the United Arab Emirates (UAE), you have the option of signing up for ASOS Premier. This service gives you free Standard and Express delivery all year round when you spend over 150 AED. Your ASOS Premier subscription will only be valid on the site you purchased it on and cannot be used for orders being delivered outside of the UAE. Please note that orders to the UAE are subject to taxes. ASOS Premier is for personal use only and is subject to a fair use policy. It costs 200 AED and is valid on the order you purchase it on.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What premier service options do I have in the UAE?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77e646",
   "metadata": {},
   "source": [
    "# YouTube Video Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89160742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube_transcript_api~=0.5.0 (from -r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/youtube_transcript/requirements.txt (line 1))\n",
      "  Downloading youtube_transcript_api-0.5.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from youtube_transcript_api~=0.5.0->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/youtube_transcript/requirements.txt (line 1)) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from requests->youtube_transcript_api~=0.5.0->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/youtube_transcript/requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from requests->youtube_transcript_api~=0.5.0->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/youtube_transcript/requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from requests->youtube_transcript_api~=0.5.0->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/youtube_transcript/requirements.txt (line 1)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages (from requests->youtube_transcript_api~=0.5.0->-r /home/joost/git/custom-knowledge-chatbot/.venv/lib/python3.10/site-packages/llama_index/readers/llamahub_modules/youtube_transcript/requirements.txt (line 1)) (2023.5.7)\n",
      "Installing collected packages: youtube_transcript_api\n",
      "Successfully installed youtube_transcript_api-0.5.0\n"
     ]
    }
   ],
   "source": [
    "YoutubeTranscriptReader = download_loader(\"YoutubeTranscriptReader\")\n",
    "\n",
    "loader = YoutubeTranscriptReader()\n",
    "documents = loader.load_data(ytlinks=['https://www.youtube.com/watch?v=K7Kh9Ntd8VE&ab_channel=DaveNick'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0995d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "194e88fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Don't steal other people's content and repost it on your own channel.\n",
      "2. Don't invest too much money in the beginning.\n",
      "3. Don't try to beat the YouTube algorithm without understanding how it works.\n",
      "4. Don't choose a niche that is too competitive.\n",
      "5. Don't create low-quality videos.\n",
      "6. Don't forget to do keyword research.\n",
      "7. Don't forget to create a thumbnail for your videos.\n",
      "8. Don't forget to promote your videos.\n",
      "9. Don't forget to set up automation systems.\n",
      "10. Don't forget to track your progress.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What some YouTube automation mistakes to avoid?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a05da",
   "metadata": {},
   "source": [
    "# Chatbot Class - Just include your index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65a0e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, api_key, index):\n",
    "        self.index = index\n",
    "        openai.api_key = api_key\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = index.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return message\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0d3da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader('./data').load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24576df3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Chatbot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Swap out your index below for whatever knowledge base you want\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m bot \u001b[39m=\u001b[39m Chatbot(\u001b[39m\"\u001b[39m\u001b[39msk-NYb192H5GW06MhN1kWt8T3BlbkFJTXKSjioslpDvlfQTYBEL\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39mindex)\n\u001b[1;32m      3\u001b[0m bot\u001b[39m.\u001b[39mload_chat_history(\u001b[39m\"\u001b[39m\u001b[39mchat_history.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Chatbot' is not defined"
     ]
    }
   ],
   "source": [
    "# Swap out your index below for whatever knowledge base you want\n",
    "bot = Chatbot(\"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\", index=index)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    response = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {response['content']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
